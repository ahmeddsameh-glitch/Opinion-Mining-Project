{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98510e74",
   "metadata": {},
   "source": [
    "# Subjectivity Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "added490",
   "metadata": {},
   "source": [
    "### 1. Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b8d4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              looove product highly recommend awesome\n",
       "1                        bad experience ever never buy\n",
       "2                      meh okay guess kinda boring tho\n",
       "3           absolutely amazing service www company com\n",
       "4                                  brand guy rock keep\n",
       "5    totally disappointed delivery late product broken\n",
       "6                      okay nothing special 310 may be\n",
       "7                              love color fit terrible\n",
       "8                 refund worth price http bad shop com\n",
       "9                           thanks brand quick support\n",
       "Name: lemmatized, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file = '../../data/test_sample.csv'\n",
    "\n",
    "# Read CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "df\n",
    "df[\"lemmatized\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315083b",
   "metadata": {},
   "source": [
    "### 2. Subjectivity Detection Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "415d751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to /home/mca/nltk_data...\n",
      "[nltk_data]   Package subjectivity is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l looove product highly recommend awesome\n",
      "sentences :  ['looove product highly recommend awesome']\n",
      "['looove', 'product', 'highly', 'recommend', 'awesome']\n",
      "after extracting features :  {'looove': True, 'product': True, 'highly': True, 'recommend': True, 'awesome': True}\n",
      "SUBJ ‚Üí looove product highly recommend awesome\n",
      "l bad experience ever never buy\n",
      "sentences :  ['bad experience ever never buy']\n",
      "['bad', 'experience', 'ever', 'never', 'buy']\n",
      "after extracting features :  {'bad': True, 'experience': True, 'ever': True, 'never': True, 'buy': True}\n",
      "SUBJ ‚Üí bad experience ever never buy\n",
      "l meh okay guess kinda boring tho\n",
      "sentences :  ['meh okay guess kinda boring tho']\n",
      "['meh', 'okay', 'guess', 'kinda', 'boring', 'tho']\n",
      "after extracting features :  {'meh': True, 'okay': True, 'guess': True, 'kinda': True, 'boring': True, 'tho': True}\n",
      "SUBJ ‚Üí meh okay guess kinda boring tho\n",
      "l absolutely amazing service www company com\n",
      "sentences :  ['absolutely amazing service www company com']\n",
      "['absolutely', 'amazing', 'service', 'www', 'company', 'com']\n",
      "after extracting features :  {'absolutely': True, 'amazing': True, 'service': True, 'www': True, 'company': True, 'com': True}\n",
      "SUBJ ‚Üí absolutely amazing service www company com\n",
      "l brand guy rock keep\n",
      "sentences :  ['brand guy rock keep']\n",
      "['brand', 'guy', 'rock', 'keep']\n",
      "after extracting features :  {'brand': True, 'guy': True, 'rock': True, 'keep': True}\n",
      " OBJ ‚Üí brand guy rock keep\n",
      "l totally disappointed delivery late product broken\n",
      "sentences :  ['totally disappointed delivery late product broken']\n",
      "['totally', 'disappointed', 'delivery', 'late', 'product', 'broken']\n",
      "after extracting features :  {'totally': True, 'disappointed': True, 'delivery': True, 'late': True, 'product': True, 'broken': True}\n",
      "SUBJ ‚Üí totally disappointed delivery late product broken\n",
      "l okay nothing special 310 may be\n",
      "sentences :  ['okay nothing special 310 may be']\n",
      "['okay', 'nothing', 'special', '310', 'may', 'be']\n",
      "after extracting features :  {'okay': True, 'nothing': True, 'special': True, '310': True, 'may': True, 'be': True}\n",
      "SUBJ ‚Üí okay nothing special 310 may be\n",
      "l love color fit terrible\n",
      "sentences :  ['love color fit terrible']\n",
      "['love', 'color', 'fit', 'terrible']\n",
      "after extracting features :  {'love': True, 'color': True, 'fit': True, 'terrible': True}\n",
      "SUBJ ‚Üí love color fit terrible\n",
      "l refund worth price http bad shop com\n",
      "sentences :  ['refund worth price http bad shop com']\n",
      "['refund', 'worth', 'price', 'http', 'bad', 'shop', 'com']\n",
      "after extracting features :  {'refund': True, 'worth': True, 'price': True, 'http': True, 'bad': True, 'shop': True, 'com': True}\n",
      " OBJ ‚Üí refund worth price http bad shop com\n",
      "l thanks brand quick support\n",
      "sentences :  ['thanks brand quick support']\n",
      "['thanks', 'brand', 'quick', 'support']\n",
      "after extracting features :  {'thanks': True, 'brand': True, 'quick': True, 'support': True}\n",
      "SUBJ ‚Üí thanks brand quick support\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>no_emoji</th>\n",
       "      <th>segmented</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>sentence_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I LOOOVE this product üòçüòç!!! Highly recommended...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i looove this product üòçüòç highly recommended aw...</td>\n",
       "      <td>loooveproductüòçüòçhighlyrecommendedawesome</td>\n",
       "      <td>loooveproducthighlyrecommendedawesome</td>\n",
       "      <td>looove product highly recommended awesome</td>\n",
       "      <td>['looove', 'product', 'highly', 'recommended',...</td>\n",
       "      <td>looove product highly recommend awesome</td>\n",
       "      <td>[subj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Worst. Experience. Ever. Will NEVER buy again!...</td>\n",
       "      <td>negative</td>\n",
       "      <td>worst experience ever will never buy again ü§Æü§¨</td>\n",
       "      <td>worstexperienceeverneverbuyü§Æü§¨</td>\n",
       "      <td>worstexperienceeverneverbuy</td>\n",
       "      <td>worst experience ever never buy</td>\n",
       "      <td>['worst', 'experience', 'ever', 'never', 'buy']</td>\n",
       "      <td>bad experience ever never buy</td>\n",
       "      <td>[subj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>meh... it was okay, I guess. kinda boring tho üôÑ</td>\n",
       "      <td>neutral</td>\n",
       "      <td>meh it was okay i guess kinda boring tho üôÑ</td>\n",
       "      <td>mehokayguesskindaboringthoüôÑ</td>\n",
       "      <td>mehokayguesskindaboringtho</td>\n",
       "      <td>meh okay guess kinda boring tho</td>\n",
       "      <td>['meh', 'okay', 'guess', 'kinda', 'boring', 't...</td>\n",
       "      <td>meh okay guess kinda boring tho</td>\n",
       "      <td>[subj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ABSOLUTELY AMAZING SERVICE!!! üòçüíØ www.company.com</td>\n",
       "      <td>positive</td>\n",
       "      <td>absolutely amazing service üòçüíØ wwwcompanycom</td>\n",
       "      <td>absolutelyamazingserviceüòçüíØwwwcompanycom</td>\n",
       "      <td>absolutelyamazingservicewwwcompanycom</td>\n",
       "      <td>absolutely amazing service www company com</td>\n",
       "      <td>['absolutely', 'amazing', 'service', 'www', 'c...</td>\n",
       "      <td>absolutely amazing service www company com</td>\n",
       "      <td>[subj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@brand You guys rock! Keep it up üëèüî•üî•üî•</td>\n",
       "      <td>positive</td>\n",
       "      <td>brand you guys rock keep it up üëèüî•üî•üî•</td>\n",
       "      <td>brandguysrockkeepüëèüî•üî•üî•</td>\n",
       "      <td>brandguysrockkeep</td>\n",
       "      <td>brand guys rock keep</td>\n",
       "      <td>['brand', 'guys', 'rock', 'keep']</td>\n",
       "      <td>brand guy rock keep</td>\n",
       "      <td>[obj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Totally disappointed. Delivery late, product b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>totally disappointed delivery late product bro...</td>\n",
       "      <td>totallydisappointeddeliverylateproductbrokenüò°</td>\n",
       "      <td>totallydisappointeddeliverylateproductbroken</td>\n",
       "      <td>totally disappointed delivery late product broken</td>\n",
       "      <td>['totally', 'disappointed', 'delivery', 'late'...</td>\n",
       "      <td>totally disappointed delivery late product broken</td>\n",
       "      <td>[subj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>just okay. nothing special. 3/10 maybe ü§∑‚Äç‚ôÇÔ∏è</td>\n",
       "      <td>neutral</td>\n",
       "      <td>just okay nothing special 310 maybe ü§∑‚Äç‚ôÇÔ∏è</td>\n",
       "      <td>okaynothingspecial310maybeü§∑‚Äç‚ôÇÔ∏è</td>\n",
       "      <td>okaynothingspecial310maybe</td>\n",
       "      <td>okay nothing special 310 may be</td>\n",
       "      <td>['okay', 'nothing', 'special', '310', 'may', '...</td>\n",
       "      <td>okay nothing special 310 may be</td>\n",
       "      <td>[subj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Loved the color, but the fit was terrible :(</td>\n",
       "      <td>negative</td>\n",
       "      <td>loved the color but the fit was terrible</td>\n",
       "      <td>lovedcolorfitterrible</td>\n",
       "      <td>lovedcolorfitterrible</td>\n",
       "      <td>loved color fit terrible</td>\n",
       "      <td>['loved', 'color', 'fit', 'terrible']</td>\n",
       "      <td>love color fit terrible</td>\n",
       "      <td>[subj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Refunded. Not worth the price!!! http://badsho...</td>\n",
       "      <td>negative</td>\n",
       "      <td>refunded not worth the price httpbadshopcom</td>\n",
       "      <td>refundedworthpricehttpbadshopcom</td>\n",
       "      <td>refundedworthpricehttpbadshopcom</td>\n",
       "      <td>refunded worth price http bad shop com</td>\n",
       "      <td>['refunded', 'worth', 'price', 'http', 'bad', ...</td>\n",
       "      <td>refund worth price http bad shop com</td>\n",
       "      <td>[obj]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Thanks @brand for the quick support!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks brand for the quick support</td>\n",
       "      <td>thanksbrandquicksupport</td>\n",
       "      <td>thanksbrandquicksupport</td>\n",
       "      <td>thanks brand quick support</td>\n",
       "      <td>['thanks', 'brand', 'quick', 'support']</td>\n",
       "      <td>thanks brand quick support</td>\n",
       "      <td>[subj]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text     label  \\\n",
       "0           0  I LOOOVE this product üòçüòç!!! Highly recommended...  positive   \n",
       "1           1  Worst. Experience. Ever. Will NEVER buy again!...  negative   \n",
       "2           2    meh... it was okay, I guess. kinda boring tho üôÑ   neutral   \n",
       "3           3   ABSOLUTELY AMAZING SERVICE!!! üòçüíØ www.company.com  positive   \n",
       "4           4              @brand You guys rock! Keep it up üëèüî•üî•üî•  positive   \n",
       "5           5  Totally disappointed. Delivery late, product b...  negative   \n",
       "6           6        just okay. nothing special. 3/10 maybe ü§∑‚Äç‚ôÇÔ∏è   neutral   \n",
       "7           7       Loved the color, but the fit was terrible :(  negative   \n",
       "8           8  Refunded. Not worth the price!!! http://badsho...  negative   \n",
       "9           9              Thanks @brand for the quick support!!  positive   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  i looove this product üòçüòç highly recommended aw...   \n",
       "1      worst experience ever will never buy again ü§Æü§¨   \n",
       "2         meh it was okay i guess kinda boring tho üôÑ   \n",
       "3        absolutely amazing service üòçüíØ wwwcompanycom   \n",
       "4                brand you guys rock keep it up üëèüî•üî•üî•   \n",
       "5  totally disappointed delivery late product bro...   \n",
       "6           just okay nothing special 310 maybe ü§∑‚Äç‚ôÇÔ∏è   \n",
       "7          loved the color but the fit was terrible    \n",
       "8        refunded not worth the price httpbadshopcom   \n",
       "9                 thanks brand for the quick support   \n",
       "\n",
       "                                    no_stopwords  \\\n",
       "0        loooveproductüòçüòçhighlyrecommendedawesome   \n",
       "1                  worstexperienceeverneverbuyü§Æü§¨   \n",
       "2                    mehokayguesskindaboringthoüôÑ   \n",
       "3        absolutelyamazingserviceüòçüíØwwwcompanycom   \n",
       "4                          brandguysrockkeepüëèüî•üî•üî•   \n",
       "5  totallydisappointeddeliverylateproductbrokenüò°   \n",
       "6                 okaynothingspecial310maybeü§∑‚Äç‚ôÇÔ∏è   \n",
       "7                          lovedcolorfitterrible   \n",
       "8               refundedworthpricehttpbadshopcom   \n",
       "9                        thanksbrandquicksupport   \n",
       "\n",
       "                                       no_emoji  \\\n",
       "0         loooveproducthighlyrecommendedawesome   \n",
       "1                   worstexperienceeverneverbuy   \n",
       "2                    mehokayguesskindaboringtho   \n",
       "3         absolutelyamazingservicewwwcompanycom   \n",
       "4                             brandguysrockkeep   \n",
       "5  totallydisappointeddeliverylateproductbroken   \n",
       "6                    okaynothingspecial310maybe   \n",
       "7                         lovedcolorfitterrible   \n",
       "8              refundedworthpricehttpbadshopcom   \n",
       "9                       thanksbrandquicksupport   \n",
       "\n",
       "                                           segmented  \\\n",
       "0          looove product highly recommended awesome   \n",
       "1                    worst experience ever never buy   \n",
       "2                    meh okay guess kinda boring tho   \n",
       "3         absolutely amazing service www company com   \n",
       "4                               brand guys rock keep   \n",
       "5  totally disappointed delivery late product broken   \n",
       "6                    okay nothing special 310 may be   \n",
       "7                           loved color fit terrible   \n",
       "8             refunded worth price http bad shop com   \n",
       "9                         thanks brand quick support   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  ['looove', 'product', 'highly', 'recommended',...   \n",
       "1    ['worst', 'experience', 'ever', 'never', 'buy']   \n",
       "2  ['meh', 'okay', 'guess', 'kinda', 'boring', 't...   \n",
       "3  ['absolutely', 'amazing', 'service', 'www', 'c...   \n",
       "4                  ['brand', 'guys', 'rock', 'keep']   \n",
       "5  ['totally', 'disappointed', 'delivery', 'late'...   \n",
       "6  ['okay', 'nothing', 'special', '310', 'may', '...   \n",
       "7              ['loved', 'color', 'fit', 'terrible']   \n",
       "8  ['refunded', 'worth', 'price', 'http', 'bad', ...   \n",
       "9            ['thanks', 'brand', 'quick', 'support']   \n",
       "\n",
       "                                          lemmatized sentence_subjectivity  \n",
       "0            looove product highly recommend awesome                [subj]  \n",
       "1                      bad experience ever never buy                [subj]  \n",
       "2                    meh okay guess kinda boring tho                [subj]  \n",
       "3         absolutely amazing service www company com                [subj]  \n",
       "4                                brand guy rock keep                 [obj]  \n",
       "5  totally disappointed delivery late product broken                [subj]  \n",
       "6                    okay nothing special 310 may be                [subj]  \n",
       "7                            love color fit terrible                [subj]  \n",
       "8               refund worth price http bad shop com                 [obj]  \n",
       "9                         thanks brand quick support                [subj]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mca/Opinion-Mining-Project/src')\n",
    "\n",
    "from tokenization_utils import tokenize_sentence \n",
    "from preprocess_text import better_sentence_splitter\n",
    "# from training_model import get_obj_docs_extra,get_subj_docs_extra\n",
    "#nltk.corpus.subjectivity is a built-in NLTK corpus that contains a dataset of subjective and objective sentences extracted from movie reviews. \n",
    "from nltk.corpus import subjectivity\n",
    "# NaiveBayesClassifier is a simple probabilistic classifier from NLTK that we use to train on those labeled sentences so it learns to classify new sentences as subjective or objective.\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "#sent_tokenize is used to split raw text (big sentences) into sentences before classifying each one.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('subjectivity')\n",
    "# We create subj_docs and obj_docs as training data for the classifier.\n",
    "# The classifier uses this data to learn how to tell if a sentence is an opinion or a fact.\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')] \n",
    "\n",
    "# - Pair features with labels.\n",
    "additional_subj_docs = [\n",
    "    (['brand', 'guy', 'rock', 'keep'], 'subj'),\n",
    "    (['thanks', 'brand', 'quick', 'support'], 'subj'),\n",
    "    (['awesome', 'job'], 'subj'),\n",
    "    (['love', 'this'], 'subj'),\n",
    "    (['great', 'service'], 'subj'),\n",
    "    (['really', 'liked', 'it'], 'subj'),\n",
    "    (['highly', 'recommended'], 'subj'),\n",
    "    (['fantastic', 'work'], 'subj'),\n",
    "    (['well', 'done'], 'subj'),\n",
    "    (['perfect', 'experience'], 'subj')\n",
    "]\n",
    "additional_subj_docs += [\n",
    "    (['excellent', 'product'], 'subj'),\n",
    "    (['very', 'happy', 'with', 'this'], 'subj'),\n",
    "    (['not', 'what', 'I', 'expected'], 'subj'),\n",
    "    (['disappointed', 'with', 'service'], 'subj'),\n",
    "    (['highly', 'suggest', 'to', 'try'], 'subj'),\n",
    "    (['would', 'buy', 'again'], 'subj'),\n",
    "    (['terrible', 'quality'], 'subj'),\n",
    "    (['best', 'purchase', 'ever'], 'subj'),\n",
    "    (['not', 'recommend'], 'subj'),\n",
    "    (['loved', 'it'], 'subj'),\n",
    "]\n",
    "subj_docs_extended = subj_docs + additional_subj_docs\n",
    "# Feature extractor example: presence of words\n",
    "def extract_features(words):\n",
    "#extract_features(['The', 'movie', 'was', 'Great'])\n",
    "# {\n",
    "#   'the': True,\n",
    "#   'movie': True,\n",
    "#   'was': True,\n",
    "#   'great': True\n",
    "# }\n",
    "    return {word.lower(): True for word in words}\n",
    "# Prepare training data\n",
    "# - Combine all subjective and objective labeled sentences.\n",
    "\n",
    "# - Convert each sentence into a feature dictionary using extract_features.\n",
    "# Combine with original subjective documents\n",
    "# - Store all pairs in train for classifier training.\n",
    "train = [(extract_features(doc), label) for doc, label in subj_docs_extended + obj_docs]\n",
    "# Train classifier\n",
    "classifier = NaiveBayesClassifier.train(train)\n",
    "all_labels = []\n",
    "for index, row in df.iterrows():\n",
    "    lemmatized_text = row['lemmatized']\n",
    "    print(\"l\",lemmatized_text)\n",
    "    sentences = better_sentence_splitter(lemmatized_text)\n",
    "    print(\"sentences : \",sentences)\n",
    "    sentence_labels = []\n",
    "    for sentence in sentences:\n",
    "        words = tokenize_sentence(sentence)   # word tokenizer\n",
    "        features = extract_features(words)\n",
    "        print(\"after extracting features : \", features)\n",
    "        label = classifier.classify(features)\n",
    "        sentence_labels.append(label)\n",
    "        print(f\"{label.upper():>4} ‚Üí {sentence}\")\n",
    "    all_labels.append(sentence_labels)\n",
    "df['sentence_subjectivity'] = all_labels\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce7538",
   "metadata": {},
   "source": [
    "### 3. Change to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b39e6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/test_sample.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
