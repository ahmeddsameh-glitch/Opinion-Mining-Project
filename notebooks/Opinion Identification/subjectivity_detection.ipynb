{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98510e74",
   "metadata": {},
   "source": [
    "# Subjectivity Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315083b",
   "metadata": {},
   "source": [
    "### Subjectivity Detection Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to /home/mca/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/subjectivity.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was fantastic but the ending was disappointing. -> subj\n"
     ]
    }
   ],
   "source": [
    "#nltk.corpus.subjectivity is a built-in NLTK corpus that contains a dataset of subjective and objective sentences extracted from movie reviews. \n",
    "from nltk.corpus import subjectivity\n",
    "# NaiveBayesClassifier is a simple probabilistic classifier from NLTK that we use to train on those labeled sentences so it learns to classify new sentences as subjective or objective.\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "#sent_tokenize is used to split raw text (big sentences) into sentences before classifying each one.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('subjectivity')\n",
    "# We create subj_docs and obj_docs as training data for the classifier.\n",
    "# The classifier uses this data to learn how to tell if a sentence is an opinion or a fact.\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')]\n",
    "\n",
    "# A small example subjective and objective dataset\n",
    "# subj_docs = [\n",
    "#     (['I', 'love', 'this', 'movie'], 'subj'),\n",
    "#     (['This', 'was', 'amazing'], 'subj')\n",
    "# ]\n",
    "# obj_docs = [\n",
    "#     (['The', 'movie', 'was', 'released', 'in', '2020'], 'obj'),\n",
    "#     (['It', 'lasts', '120', 'minutes'], 'obj')\n",
    "# ]\n",
    "# Feature extractor example: presence of words\n",
    "def extract_features(words):\n",
    "#extract_features(['The', 'movie', 'was', 'Great'])\n",
    "# {\n",
    "#   'the': True,\n",
    "#   'movie': True,\n",
    "#   'was': True,\n",
    "#   'great': True\n",
    "# }\n",
    "    return {word.lower(): True for word in words}\n",
    "# Prepare training data\n",
    "# - Combine all subjective and objective labeled sentences.\n",
    "\n",
    "# - Convert each sentence into a feature dictionary using extract_features.\n",
    "\n",
    "# - Pair features with labels.\n",
    "\n",
    "# - Store all pairs in train for classifier training.\n",
    "train = [(extract_features(doc), label) for doc, label in subj_docs + obj_docs]\n",
    "\n",
    "# Train classifier\n",
    "classifier = NaiveBayesClassifier.train(train)\n",
    "# Example text\n",
    "text = \"The movie was fantastic but the ending was disappointing.\"\n",
    "\n",
    "# Split to sentences and classify\n",
    "for sentence in sent_tokenize(text):\n",
    "    features = extract_features(sentence.split())\n",
    "    print(\"after extracting features : \", features)\n",
    "    print(sentence, '->', classifier.classify(features))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
