{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98510e74",
   "metadata": {},
   "source": [
    "# Subjectivity Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "added490",
   "metadata": {},
   "source": [
    "### 1. Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b8d4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              looove product highly recommend awesome\n",
       "1                        bad experience ever never buy\n",
       "2                      meh okay guess kinda boring tho\n",
       "3           absolutely amazing service www company com\n",
       "4                                  brand guy rock keep\n",
       "5    totally disappointed delivery late product broken\n",
       "6                      okay nothing special 310 may be\n",
       "7                              love color fit terrible\n",
       "8                 refund worth price http bad shop com\n",
       "9                           thanks brand quick support\n",
       "Name: lemmatized, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file = '../../data/test_sample.csv'\n",
    "\n",
    "# Read CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "df\n",
    "df[\"lemmatized\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315083b",
   "metadata": {},
   "source": [
    "### 2. Subjectivity Detection Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to /home/mca/nltk_data...\n",
      "[nltk_data]   Package subjectivity is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l looove product highly recommend awesome\n",
      "sentences :  ['looove product highly recommend awesome']\n",
      "['looove', 'product', 'highly', 'recommend', 'awesome']\n",
      "after extracting features :  {'looove': True, 'product': True, 'highly': True, 'recommend': True, 'awesome': True}\n",
      "SUBJ → looove product highly recommend awesome\n",
      "l bad experience ever never buy\n",
      "sentences :  ['bad experience ever never buy']\n",
      "['bad', 'experience', 'ever', 'never', 'buy']\n",
      "after extracting features :  {'bad': True, 'experience': True, 'ever': True, 'never': True, 'buy': True}\n",
      "SUBJ → bad experience ever never buy\n",
      "l meh okay guess kinda boring tho\n",
      "sentences :  ['meh okay guess kinda boring tho']\n",
      "['meh', 'okay', 'guess', 'kinda', 'boring', 'tho']\n",
      "after extracting features :  {'meh': True, 'okay': True, 'guess': True, 'kinda': True, 'boring': True, 'tho': True}\n",
      "SUBJ → meh okay guess kinda boring tho\n",
      "l absolutely amazing service www company com\n",
      "sentences :  ['absolutely amazing service www company com']\n",
      "['absolutely', 'amazing', 'service', 'www', 'company', 'com']\n",
      "after extracting features :  {'absolutely': True, 'amazing': True, 'service': True, 'www': True, 'company': True, 'com': True}\n",
      "SUBJ → absolutely amazing service www company com\n",
      "l brand guy rock keep\n",
      "sentences :  ['brand guy rock keep']\n",
      "['brand', 'guy', 'rock', 'keep']\n",
      "after extracting features :  {'brand': True, 'guy': True, 'rock': True, 'keep': True}\n",
      " OBJ → brand guy rock keep\n",
      "l totally disappointed delivery late product broken\n",
      "sentences :  ['totally disappointed delivery late product broken']\n",
      "['totally', 'disappointed', 'delivery', 'late', 'product', 'broken']\n",
      "after extracting features :  {'totally': True, 'disappointed': True, 'delivery': True, 'late': True, 'product': True, 'broken': True}\n",
      "SUBJ → totally disappointed delivery late product broken\n",
      "l okay nothing special 310 may be\n",
      "sentences :  ['okay nothing special 310 may be']\n",
      "['okay', 'nothing', 'special', '310', 'may', 'be']\n",
      "after extracting features :  {'okay': True, 'nothing': True, 'special': True, '310': True, 'may': True, 'be': True}\n",
      "SUBJ → okay nothing special 310 may be\n",
      "l love color fit terrible\n",
      "sentences :  ['love color fit terrible']\n",
      "['love', 'color', 'fit', 'terrible']\n",
      "after extracting features :  {'love': True, 'color': True, 'fit': True, 'terrible': True}\n",
      "SUBJ → love color fit terrible\n",
      "l refund worth price http bad shop com\n",
      "sentences :  ['refund worth price http bad shop com']\n",
      "['refund', 'worth', 'price', 'http', 'bad', 'shop', 'com']\n",
      "after extracting features :  {'refund': True, 'worth': True, 'price': True, 'http': True, 'bad': True, 'shop': True, 'com': True}\n",
      " OBJ → refund worth price http bad shop com\n",
      "l thanks brand quick support\n",
      "sentences :  ['thanks brand quick support']\n",
      "['thanks', 'brand', 'quick', 'support']\n",
      "after extracting features :  {'thanks': True, 'brand': True, 'quick': True, 'support': True}\n",
      "SUBJ → thanks brand quick support\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mca/Opinion-Mining-Project/src')\n",
    "\n",
    "from tokenization_utils import tokenize_sentence \n",
    "from preprocess_text import better_sentence_splitter\n",
    "#nltk.corpus.subjectivity is a built-in NLTK corpus that contains a dataset of subjective and objective sentences extracted from movie reviews. \n",
    "from nltk.corpus import subjectivity\n",
    "# NaiveBayesClassifier is a simple probabilistic classifier from NLTK that we use to train on those labeled sentences so it learns to classify new sentences as subjective or objective.\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "#sent_tokenize is used to split raw text (big sentences) into sentences before classifying each one.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('subjectivity')\n",
    "# We create subj_docs and obj_docs as training data for the classifier.\n",
    "# The classifier uses this data to learn how to tell if a sentence is an opinion or a fact.\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')] \n",
    "# A small example subjective and objective dataset\n",
    "# subj_docs = [\n",
    "#     (['I', 'love', 'this', 'movie'], 'subj'),\n",
    "#     (['This', 'was', 'amazing'], 'subj')\n",
    "# ]\n",
    "# obj_docs = [\n",
    "#     (['The', 'movie', 'was', 'released', 'in', '2020'], 'obj'),\n",
    "#     (['It', 'lasts', '120', 'minutes'], 'obj')\n",
    "# ]\n",
    "# Feature extractor example: presence of words\n",
    "def extract_features(words):\n",
    "#extract_features(['The', 'movie', 'was', 'Great'])\n",
    "# {\n",
    "#   'the': True,\n",
    "#   'movie': True,\n",
    "#   'was': True,\n",
    "#   'great': True\n",
    "# }\n",
    "    return {word.lower(): True for word in words}\n",
    "# Prepare training data\n",
    "# - Combine all subjective and objective labeled sentences.\n",
    "\n",
    "# - Convert each sentence into a feature dictionary using extract_features.\n",
    "\n",
    "# - Pair features with labels.\n",
    "additional_subj_docs = [\n",
    "    (['brand', 'guy', 'rock', 'keep'], 'subj'),\n",
    "    (['thanks', 'brand', 'quick', 'support'], 'subj'),\n",
    "    (['awesome', 'job'], 'subj'),\n",
    "    (['love', 'this'], 'subj'),\n",
    "    (['great', 'service'], 'subj'),\n",
    "    (['really', 'liked', 'it'], 'subj'),\n",
    "    (['highly', 'recommended'], 'subj'),\n",
    "    (['fantastic', 'work'], 'subj'),\n",
    "    (['well', 'done'], 'subj'),\n",
    "    (['perfect', 'experience'], 'subj')\n",
    "]\n",
    "additional_subj_docs += [\n",
    "    (['excellent', 'product'], 'subj'),\n",
    "    (['very', 'happy', 'with', 'this'], 'subj'),\n",
    "    (['not', 'what', 'I', 'expected'], 'subj'),\n",
    "    (['disappointed', 'with', 'service'], 'subj'),\n",
    "    (['highly', 'suggest', 'to', 'try'], 'subj'),\n",
    "    (['would', 'buy', 'again'], 'subj'),\n",
    "    (['terrible', 'quality'], 'subj'),\n",
    "    (['best', 'purchase', 'ever'], 'subj'),\n",
    "    (['not', 'recommend'], 'subj'),\n",
    "    (['loved', 'it'], 'subj'),\n",
    "]\n",
    "\n",
    "# Combine with original subjective documents\n",
    "subj_docs_extended = subj_docs + additional_subj_docs\n",
    "# - Store all pairs in train for classifier training.\n",
    "train = [(extract_features(doc), label) for doc, label in subj_docs_extended + obj_docs]\n",
    "# Train classifier\n",
    "classifier = NaiveBayesClassifier.train(train)\n",
    "results = []\n",
    "for index, row in df.iterrows():\n",
    "    lemmatized_text = row['lemmatized']\n",
    "    print(\"l\",lemmatized_text)\n",
    "    sentences = better_sentence_splitter(lemmatized_text)\n",
    "    print(\"sentences : \",sentences)\n",
    "    for sentence in sentences:\n",
    "        words = tokenize_sentence(sentence)   # word tokenizer\n",
    "        features = extract_features(words)\n",
    "        print(\"after extracting features : \", features)\n",
    "        label = classifier.classify(features)\n",
    "        print(f\"{label.upper():>4} → {sentence}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
