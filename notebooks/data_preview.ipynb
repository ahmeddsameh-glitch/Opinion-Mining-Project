{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b93828c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d17f6eca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:41:38.697050Z",
     "start_time": "2025-07-25T06:41:38.443177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I LOOOVE this product 😍😍!!! Highly recommended...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst. Experience. Ever. Will NEVER buy again!...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meh... it was okay, I guess. kinda boring tho 🙄</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABSOLUTELY AMAZING SERVICE!!! 😍💯 www.company.com</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@brand You guys rock! Keep it up 👏🔥🔥🔥</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Totally disappointed. Delivery late, product b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>just okay. nothing special. 3/10 maybe 🤷‍♂️</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Loved the color, but the fit was terrible :(</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Refunded. Not worth the price!!! http://badsho...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thanks @brand for the quick support!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  I LOOOVE this product 😍😍!!! Highly recommended...  positive\n",
       "1  Worst. Experience. Ever. Will NEVER buy again!...  negative\n",
       "2    meh... it was okay, I guess. kinda boring tho 🙄   neutral\n",
       "3   ABSOLUTELY AMAZING SERVICE!!! 😍💯 www.company.com  positive\n",
       "4              @brand You guys rock! Keep it up 👏🔥🔥🔥  positive\n",
       "5  Totally disappointed. Delivery late, product b...  negative\n",
       "6        just okay. nothing special. 3/10 maybe 🤷‍♂️   neutral\n",
       "7       Loved the color, but the fit was terrible :(  negative\n",
       "8  Refunded. Not worth the price!!! http://badsho...  negative\n",
       "9              Thanks @brand for the quick support!!  positive"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"I LOOOVE this product 😍😍!!! Highly recommended... #awesome\",\n",
    "        \"Worst. Experience. Ever. Will NEVER buy again!!! 🤮🤬\",\n",
    "        \"meh... it was okay, I guess. kinda boring tho 🙄\",\n",
    "        \"ABSOLUTELY AMAZING SERVICE!!! 😍💯 www.company.com\",\n",
    "        \"@brand You guys rock! Keep it up 👏🔥🔥🔥\",\n",
    "        \"Totally disappointed. Delivery late, product broken. 😡\",\n",
    "        \"just okay. nothing special. 3/10 maybe 🤷‍♂️\",\n",
    "        \"Loved the color, but the fit was terrible :(\",\n",
    "        \"Refunded. Not worth the price!!! http://badshop.com\",\n",
    "        \"Thanks @brand for the quick support!!\"\n",
    "    ],\n",
    "    \"label\": [\n",
    "        \"positive\", \"negative\", \"neutral\", \"positive\", \"positive\",\n",
    "        \"negative\", \"neutral\", \"negative\", \"negative\", \"positive\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data) \n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e6a64",
   "metadata": {},
   "source": [
    "### 2. Change to a csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8911086",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:41:44.118597Z",
     "start_time": "2025-07-25T06:41:44.109844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I LOOOVE this product 😍😍!!! Highly recommended...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst. Experience. Ever. Will NEVER buy again!...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meh... it was okay, I guess. kinda boring tho 🙄</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABSOLUTELY AMAZING SERVICE!!! 😍💯 www.company.com</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@brand You guys rock! Keep it up 👏🔥🔥🔥</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  I LOOOVE this product 😍😍!!! Highly recommended...  positive\n",
       "1  Worst. Experience. Ever. Will NEVER buy again!...  negative\n",
       "2    meh... it was okay, I guess. kinda boring tho 🙄   neutral\n",
       "3   ABSOLUTELY AMAZING SERVICE!!! 😍💯 www.company.com  positive\n",
       "4              @brand You guys rock! Keep it up 👏🔥🔥🔥  positive"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('../data/test_sample.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35116134",
   "metadata": {},
   "source": [
    "### 3. Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d63f224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:41:48.410939Z",
     "start_time": "2025-07-25T06:41:48.403873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I LOOOVE this product 😍😍!!! Highly recommended...</td>\n",
       "      <td>i looove this product 😍😍!!! highly recommended...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst. Experience. Ever. Will NEVER buy again!...</td>\n",
       "      <td>worst. experience. ever. will never buy again!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meh... it was okay, I guess. kinda boring tho 🙄</td>\n",
       "      <td>meh... it was okay, i guess. kinda boring tho 🙄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABSOLUTELY AMAZING SERVICE!!! 😍💯 www.company.com</td>\n",
       "      <td>absolutely amazing service!!! 😍💯 www.company.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@brand You guys rock! Keep it up 👏🔥🔥🔥</td>\n",
       "      <td>@brand you guys rock! keep it up 👏🔥🔥🔥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Totally disappointed. Delivery late, product b...</td>\n",
       "      <td>totally disappointed. delivery late, product b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>just okay. nothing special. 3/10 maybe 🤷‍♂️</td>\n",
       "      <td>just okay. nothing special. 3/10 maybe 🤷‍♂️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Loved the color, but the fit was terrible :(</td>\n",
       "      <td>loved the color, but the fit was terrible :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Refunded. Not worth the price!!! http://badsho...</td>\n",
       "      <td>refunded. not worth the price!!! http://badsho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thanks @brand for the quick support!!</td>\n",
       "      <td>thanks @brand for the quick support!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  I LOOOVE this product 😍😍!!! Highly recommended...   \n",
       "1  Worst. Experience. Ever. Will NEVER buy again!...   \n",
       "2    meh... it was okay, I guess. kinda boring tho 🙄   \n",
       "3   ABSOLUTELY AMAZING SERVICE!!! 😍💯 www.company.com   \n",
       "4              @brand You guys rock! Keep it up 👏🔥🔥🔥   \n",
       "5  Totally disappointed. Delivery late, product b...   \n",
       "6        just okay. nothing special. 3/10 maybe 🤷‍♂️   \n",
       "7       Loved the color, but the fit was terrible :(   \n",
       "8  Refunded. Not worth the price!!! http://badsho...   \n",
       "9              Thanks @brand for the quick support!!   \n",
       "\n",
       "                                          clean_text  \n",
       "0  i looove this product 😍😍!!! highly recommended...  \n",
       "1  worst. experience. ever. will never buy again!...  \n",
       "2    meh... it was okay, i guess. kinda boring tho 🙄  \n",
       "3   absolutely amazing service!!! 😍💯 www.company.com  \n",
       "4              @brand you guys rock! keep it up 👏🔥🔥🔥  \n",
       "5  totally disappointed. delivery late, product b...  \n",
       "6        just okay. nothing special. 3/10 maybe 🤷‍♂️  \n",
       "7       loved the color, but the fit was terrible :(  \n",
       "8  refunded. not worth the price!!! http://badsho...  \n",
       "9              thanks @brand for the quick support!!  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] \n",
    "print(type(df['text']))  \n",
    "df['clean_text'] = df['text'].apply(lambda word:word.lower())\n",
    "df[['text', 'clean_text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426dcdce",
   "metadata": {},
   "source": [
    "###  4. Remove Punctuation and Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cab41b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:41:55.233369Z",
     "start_time": "2025-07-25T06:41:55.229058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    i looove this product 😍😍 highly recommended aw...\n",
       "1        worst experience ever will never buy again 🤮🤬\n",
       "2           meh it was okay i guess kinda boring tho 🙄\n",
       "3          absolutely amazing service 😍💯 wwwcompanycom\n",
       "4                  brand you guys rock keep it up 👏🔥🔥🔥\n",
       "5    totally disappointed delivery late product bro...\n",
       "6             just okay nothing special 310 maybe 🤷‍♂️\n",
       "7            loved the color but the fit was terrible \n",
       "8          refunded not worth the price httpbadshopcom\n",
       "9                   thanks brand for the quick support\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "mytable = str.maketrans('','',string.punctuation)\n",
    "df['clean_text'] = df['clean_text'].apply(lambda word : word.translate(mytable)) \n",
    "df['clean_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f365c7",
   "metadata": {},
   "source": [
    "### 5. Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1fb0370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:41:58.297658Z",
     "start_time": "2025-07-25T06:41:57.416841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I LOOOVE this product 😍😍!!! Highly recommended...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i looove this product 😍😍 highly recommended aw...</td>\n",
       "      <td>loooveproduct😍😍highlyrecommendedawesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst. Experience. Ever. Will NEVER buy again!...</td>\n",
       "      <td>negative</td>\n",
       "      <td>worst experience ever will never buy again 🤮🤬</td>\n",
       "      <td>worstexperienceeverneverbuy🤮🤬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meh... it was okay, I guess. kinda boring tho 🙄</td>\n",
       "      <td>neutral</td>\n",
       "      <td>meh it was okay i guess kinda boring tho 🙄</td>\n",
       "      <td>mehokayguesskindaboringtho🙄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABSOLUTELY AMAZING SERVICE!!! 😍💯 www.company.com</td>\n",
       "      <td>positive</td>\n",
       "      <td>absolutely amazing service 😍💯 wwwcompanycom</td>\n",
       "      <td>absolutelyamazingservice😍💯wwwcompanycom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@brand You guys rock! Keep it up 👏🔥🔥🔥</td>\n",
       "      <td>positive</td>\n",
       "      <td>brand you guys rock keep it up 👏🔥🔥🔥</td>\n",
       "      <td>brandguysrockkeep👏🔥🔥🔥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Totally disappointed. Delivery late, product b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>totally disappointed delivery late product bro...</td>\n",
       "      <td>totallydisappointeddeliverylateproductbroken😡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>just okay. nothing special. 3/10 maybe 🤷‍♂️</td>\n",
       "      <td>neutral</td>\n",
       "      <td>just okay nothing special 310 maybe 🤷‍♂️</td>\n",
       "      <td>okaynothingspecial310maybe🤷‍♂️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Loved the color, but the fit was terrible :(</td>\n",
       "      <td>negative</td>\n",
       "      <td>loved the color but the fit was terrible</td>\n",
       "      <td>lovedcolorfitterrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Refunded. Not worth the price!!! http://badsho...</td>\n",
       "      <td>negative</td>\n",
       "      <td>refunded not worth the price httpbadshopcom</td>\n",
       "      <td>refundedworthpricehttpbadshopcom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thanks @brand for the quick support!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks brand for the quick support</td>\n",
       "      <td>thanksbrandquicksupport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label  \\\n",
       "0  I LOOOVE this product 😍😍!!! Highly recommended...  positive   \n",
       "1  Worst. Experience. Ever. Will NEVER buy again!...  negative   \n",
       "2    meh... it was okay, I guess. kinda boring tho 🙄   neutral   \n",
       "3   ABSOLUTELY AMAZING SERVICE!!! 😍💯 www.company.com  positive   \n",
       "4              @brand You guys rock! Keep it up 👏🔥🔥🔥  positive   \n",
       "5  Totally disappointed. Delivery late, product b...  negative   \n",
       "6        just okay. nothing special. 3/10 maybe 🤷‍♂️   neutral   \n",
       "7       Loved the color, but the fit was terrible :(  negative   \n",
       "8  Refunded. Not worth the price!!! http://badsho...  negative   \n",
       "9              Thanks @brand for the quick support!!  positive   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  i looove this product 😍😍 highly recommended aw...   \n",
       "1      worst experience ever will never buy again 🤮🤬   \n",
       "2         meh it was okay i guess kinda boring tho 🙄   \n",
       "3        absolutely amazing service 😍💯 wwwcompanycom   \n",
       "4                brand you guys rock keep it up 👏🔥🔥🔥   \n",
       "5  totally disappointed delivery late product bro...   \n",
       "6           just okay nothing special 310 maybe 🤷‍♂️   \n",
       "7          loved the color but the fit was terrible    \n",
       "8        refunded not worth the price httpbadshopcom   \n",
       "9                 thanks brand for the quick support   \n",
       "\n",
       "                                    no_stopwords  \n",
       "0        loooveproduct😍😍highlyrecommendedawesome  \n",
       "1                  worstexperienceeverneverbuy🤮🤬  \n",
       "2                    mehokayguesskindaboringtho🙄  \n",
       "3        absolutelyamazingservice😍💯wwwcompanycom  \n",
       "4                          brandguysrockkeep👏🔥🔥🔥  \n",
       "5  totallydisappointeddeliverylateproductbroken😡  \n",
       "6                 okaynothingspecial310maybe🤷‍♂️  \n",
       "7                          lovedcolorfitterrible  \n",
       "8               refundedworthpricehttpbadshopcom  \n",
       "9                        thanksbrandquicksupport  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['no_stopwords'] = df['clean_text'].apply(\n",
    "  lambda text :  ''.join(word for word in text.split() if word not in stop_words)\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d708e0b",
   "metadata": {},
   "source": [
    "### 6. Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "851f9de1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:42:31.206016Z",
     "start_time": "2025-07-25T06:42:31.198243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>no_emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I LOOOVE this product 😍😍!!! Highly recommended...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i looove this product 😍😍 highly recommended aw...</td>\n",
       "      <td>loooveproduct😍😍highlyrecommendedawesome</td>\n",
       "      <td>loooveproducthighlyrecommendedawesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst. Experience. Ever. Will NEVER buy again!...</td>\n",
       "      <td>negative</td>\n",
       "      <td>worst experience ever will never buy again 🤮🤬</td>\n",
       "      <td>worstexperienceeverneverbuy🤮🤬</td>\n",
       "      <td>worstexperienceeverneverbuy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meh... it was okay, I guess. kinda boring tho 🙄</td>\n",
       "      <td>neutral</td>\n",
       "      <td>meh it was okay i guess kinda boring tho 🙄</td>\n",
       "      <td>mehokayguesskindaboringtho🙄</td>\n",
       "      <td>mehokayguesskindaboringtho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABSOLUTELY AMAZING SERVICE!!! 😍💯 www.company.com</td>\n",
       "      <td>positive</td>\n",
       "      <td>absolutely amazing service 😍💯 wwwcompanycom</td>\n",
       "      <td>absolutelyamazingservice😍💯wwwcompanycom</td>\n",
       "      <td>absolutelyamazingservicewwwcompanycom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@brand You guys rock! Keep it up 👏🔥🔥🔥</td>\n",
       "      <td>positive</td>\n",
       "      <td>brand you guys rock keep it up 👏🔥🔥🔥</td>\n",
       "      <td>brandguysrockkeep👏🔥🔥🔥</td>\n",
       "      <td>brandguysrockkeep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Totally disappointed. Delivery late, product b...</td>\n",
       "      <td>negative</td>\n",
       "      <td>totally disappointed delivery late product bro...</td>\n",
       "      <td>totallydisappointeddeliverylateproductbroken😡</td>\n",
       "      <td>totallydisappointeddeliverylateproductbroken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>just okay. nothing special. 3/10 maybe 🤷‍♂️</td>\n",
       "      <td>neutral</td>\n",
       "      <td>just okay nothing special 310 maybe 🤷‍♂️</td>\n",
       "      <td>okaynothingspecial310maybe🤷‍♂️</td>\n",
       "      <td>okaynothingspecial310maybe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Loved the color, but the fit was terrible :(</td>\n",
       "      <td>negative</td>\n",
       "      <td>loved the color but the fit was terrible</td>\n",
       "      <td>lovedcolorfitterrible</td>\n",
       "      <td>lovedcolorfitterrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Refunded. Not worth the price!!! http://badsho...</td>\n",
       "      <td>negative</td>\n",
       "      <td>refunded not worth the price httpbadshopcom</td>\n",
       "      <td>refundedworthpricehttpbadshopcom</td>\n",
       "      <td>refundedworthpricehttpbadshopcom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thanks @brand for the quick support!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks brand for the quick support</td>\n",
       "      <td>thanksbrandquicksupport</td>\n",
       "      <td>thanksbrandquicksupport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label  \\\n",
       "0  I LOOOVE this product 😍😍!!! Highly recommended...  positive   \n",
       "1  Worst. Experience. Ever. Will NEVER buy again!...  negative   \n",
       "2    meh... it was okay, I guess. kinda boring tho 🙄   neutral   \n",
       "3   ABSOLUTELY AMAZING SERVICE!!! 😍💯 www.company.com  positive   \n",
       "4              @brand You guys rock! Keep it up 👏🔥🔥🔥  positive   \n",
       "5  Totally disappointed. Delivery late, product b...  negative   \n",
       "6        just okay. nothing special. 3/10 maybe 🤷‍♂️   neutral   \n",
       "7       Loved the color, but the fit was terrible :(  negative   \n",
       "8  Refunded. Not worth the price!!! http://badsho...  negative   \n",
       "9              Thanks @brand for the quick support!!  positive   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  i looove this product 😍😍 highly recommended aw...   \n",
       "1      worst experience ever will never buy again 🤮🤬   \n",
       "2         meh it was okay i guess kinda boring tho 🙄   \n",
       "3        absolutely amazing service 😍💯 wwwcompanycom   \n",
       "4                brand you guys rock keep it up 👏🔥🔥🔥   \n",
       "5  totally disappointed delivery late product bro...   \n",
       "6           just okay nothing special 310 maybe 🤷‍♂️   \n",
       "7          loved the color but the fit was terrible    \n",
       "8        refunded not worth the price httpbadshopcom   \n",
       "9                 thanks brand for the quick support   \n",
       "\n",
       "                                    no_stopwords  \\\n",
       "0        loooveproduct😍😍highlyrecommendedawesome   \n",
       "1                  worstexperienceeverneverbuy🤮🤬   \n",
       "2                    mehokayguesskindaboringtho🙄   \n",
       "3        absolutelyamazingservice😍💯wwwcompanycom   \n",
       "4                          brandguysrockkeep👏🔥🔥🔥   \n",
       "5  totallydisappointeddeliverylateproductbroken😡   \n",
       "6                 okaynothingspecial310maybe🤷‍♂️   \n",
       "7                          lovedcolorfitterrible   \n",
       "8               refundedworthpricehttpbadshopcom   \n",
       "9                        thanksbrandquicksupport   \n",
       "\n",
       "                                       no_emoji  \n",
       "0         loooveproducthighlyrecommendedawesome  \n",
       "1                   worstexperienceeverneverbuy  \n",
       "2                    mehokayguesskindaboringtho  \n",
       "3         absolutelyamazingservicewwwcompanycom  \n",
       "4                             brandguysrockkeep  \n",
       "5  totallydisappointeddeliverylateproductbroken  \n",
       "6                    okaynothingspecial310maybe  \n",
       "7                         lovedcolorfitterrible  \n",
       "8              refundedworthpricehttpbadshopcom  \n",
       "9                       thanksbrandquicksupport  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "df['no_emoji'] = df['no_stopwords'].apply(lambda row: emoji.replace_emoji(row, replace=''))\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd1831",
   "metadata": {},
   "source": [
    "### 6. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bee010",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/mca/nltk_data'\n    - '/home/mca/Opinion-Mining-Project/venv/nltk_data'\n    - '/home/mca/Opinion-Mining-Project/venv/share/nltk_data'\n    - '/home/mca/Opinion-Mining-Project/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/mca/nltk_data'\n    - '/home/mca/nltk_data'\n    - '/home/mca/nltk_data'\n    - '/home/mca/nltk_data'\n    - '/home/mca/nltk_data'\n    - '/home/mca/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33msegmented\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mno_emoji\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m text: \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(segment(text)))\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Step 2: Tokenize the segmented text\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtokenized\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msegmented\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Opinion-Mining-Project/venv/lib/python3.11/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Opinion-Mining-Project/venv/lib/python3.11/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Opinion-Mining-Project/venv/lib/python3.11/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Opinion-Mining-Project/venv/lib/python3.11/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Opinion-Mining-Project/venv/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      5\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33msegmented\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mno_emoji\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m text: \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(segment(text)))\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Step 2: Tokenize the segmented text\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtokenized\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33msegmented\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m text: \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Opinion-Mining-Project/venv/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Opinion-Mining-Project/venv/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Opinion-Mining-Project/venv/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Opinion-Mining-Project/venv/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Opinion-Mining-Project/venv/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Opinion-Mining-Project/venv/lib/python3.11/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/mca/nltk_data'\n    - '/home/mca/Opinion-Mining-Project/venv/nltk_data'\n    - '/home/mca/Opinion-Mining-Project/venv/share/nltk_data'\n    - '/home/mca/Opinion-Mining-Project/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/mca/nltk_data'\n    - '/home/mca/nltk_data'\n    - '/home/mca/nltk_data'\n    - '/home/mca/nltk_data'\n    - '/home/mca/nltk_data'\n    - '/home/mca/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from wordsegment import load, segment\n",
    "\n",
    "# Force nltk to use the correct data path\n",
    "nltk.download('punkt', download_dir='/home/mca/nltk_data')\n",
    "nltk.data.path.append('/home/mca/nltk_data')\n",
    "load()\n",
    "# Step 1: Segment concatenated words into proper spacing\n",
    "df['segmented'] = df['no_emoji'].apply(lambda text: ' '.join(segment(text)))\n",
    "# Step 2: Tokenize the segmented text\n",
    "df['tokenized'] = df['segmented'].apply(lambda text: word_tokenize(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
