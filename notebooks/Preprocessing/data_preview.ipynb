{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b93828c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Gathering Data and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d11234",
   "metadata": {},
   "source": [
    "### 1. Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f6eca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:41:38.697050Z",
     "start_time": "2025-07-25T06:41:38.443177Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mca/Opinion-Mining-Project/src')\n",
    "from testdata import get_data\n",
    "\n",
    "df = get_data()\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e6a64",
   "metadata": {},
   "source": [
    "### 2. Change to a csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8911086",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:41:44.118597Z",
     "start_time": "2025-07-25T06:41:44.109844Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('../../data/test_sample.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35116134",
   "metadata": {},
   "source": [
    "### 3. Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63f224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:41:48.410939Z",
     "start_time": "2025-07-25T06:41:48.403873Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] \n",
    "print(type(df['text']))  \n",
    "df['clean_text'] = df['text'].apply(lambda word:word.lower())\n",
    "df[['text', 'clean_text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426dcdce",
   "metadata": {},
   "source": [
    "###  4. Remove Punctuation and Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab41b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:41:55.233369Z",
     "start_time": "2025-07-25T06:41:55.229058Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "mytable = str.maketrans('','',string.punctuation)\n",
    "df['clean_text'] = df['clean_text'].apply(lambda word : word.translate(mytable)) \n",
    "df['clean_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f365c7",
   "metadata": {},
   "source": [
    "### 5. Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb0370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:41:58.297658Z",
     "start_time": "2025-07-25T06:41:57.416841Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['no_stopwords'] = df['clean_text'].apply(\n",
    "  lambda text :  ''.join(word for word in text.split() if word not in stop_words)\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d708e0b",
   "metadata": {},
   "source": [
    "### 6. Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f9de1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T06:42:31.206016Z",
     "start_time": "2025-07-25T06:42:31.198243Z"
    }
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "df['no_emoji'] = df['no_stopwords'].apply(lambda row: emoji.replace_emoji(row, replace=''))\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd1831",
   "metadata": {},
   "source": [
    "### 7. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bee010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordsegment import load, segment\n",
    "\n",
    "# Download 'punkt' to a custom directory and append it to nltk path\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize wordsegment\n",
    "load()\n",
    "# Check for NaN or empty strings\n",
    "df['no_emoji'] = df['no_emoji'].fillna('').astype(str)\n",
    "print(df['no_emoji'].head())  # Confirm it prints strings like 'absolutelyamazingservice...'\n",
    "\n",
    "df['segmented'] = df['no_emoji'].apply(lambda text: ' '.join(segment(text)))\n",
    "tokenizer = lambda text: re.findall(r'\\b\\w+\\b', text.lower())\n",
    "df['tokenized'] = df['segmented'].apply(tokenizer)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e4aac",
   "metadata": {},
   "source": [
    "### 8. Lemmatization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    # Map POS tag to WordNet POS tag for lemmatizer\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default\n",
    "def lemmatize_tokens(token_list):\n",
    "    #Takes a list of words (tokens) from your sentence.\n",
    "    #Assigns a Part-of-Speech (POS) tag to each word, like noun, verb, adjective, etc.\n",
    "    #Returns a list of tuples: (word, POS_tag) for every token.\n",
    "    pos_tags = pos_tag(token_list)\n",
    "    print(\"POS tag : \",pos_tags)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    print(lemmatizer)\n",
    "    lemmatized_sentence = []\n",
    "    for token, tag in pos_tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(token, wn_tag)\n",
    "        lemmatized_sentence.append(lemma)\n",
    "    \n",
    "    print(\"Original:\", token_list)\n",
    "    print(\"Lemmatized:\", \" \".join(lemmatized_sentence))\n",
    "    # \"bats\" → \"bat\"\n",
    "\n",
    "    # \"are\" → \"be\"\n",
    "\n",
    "    # \"hanging\" → \"hang\"\n",
    "\n",
    "    # \"feet\" → \"foot\"    \n",
    "    return lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2818b89",
   "metadata": {},
   "source": [
    "### 9. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0611440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Apply lemmatization\n",
    "df['lemmatized'] = df['tokenized'].apply(lemmatize_tokens)\n",
    "df['lemmatized'] = df['lemmatized'].apply(lambda tokens: \" \".join(tokens))\n",
    "df['lemmatized'] \n",
    "df\n",
    "df.to_csv('../../data/test_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a69147",
   "metadata": {},
   "source": [
    "### Tokenization Function   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from wordsegment import load, segment\n",
    "\n",
    "# Load wordsegment once\n",
    "load()\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    # Step 1: Segment merged words (e.g., \"amazingsupport\" → \"amazing support\")\n",
    "    segmented = ' '.join(segment(sentence))\n",
    "    \n",
    "    # Step 2: Tokenize (split into lowercase words)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', segmented.lower())\n",
    "    \n",
    "    return tokens\n",
    "text = \"I recently watched the new action movie and honestly, it was thrilling from start to finish. The plot kept me on the edge of my seat, and the acting was top-notch. However, I felt the soundtrack didn’t quite match the intensity of the scenes. The movie was released in May 2023 and runs for approximately 2 hours and 15 minutes. It grossed over $300 million worldwide in its first week. Many critics praised the director for his bold choices, while some audiences thought the pacing was too fast. Personally, I think the cinematography was stunning, especially in the desert scenes. According to IMDb, the film has a rating of 7.8 out of 10.The lead actor previously starred in a hit sci-fi franchise.In my opinion, this is his best performance yet.\"\n",
    "tokenize_sentence(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
